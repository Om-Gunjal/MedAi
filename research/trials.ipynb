{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a95275ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71e378c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd \"C:\\Users\\Om gunjal\\OneDrive\\Documents\\vs\\MedAI\\MedAi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c654e3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab211f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Data From the PDF File\n",
    "def load_pdf_file(data):\n",
    "    loader= DirectoryLoader(data,\n",
    "                            glob=\"*.pdf\",\n",
    "                            loader_cls=PyPDFLoader)\n",
    "\n",
    "    documents=loader.load()\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5422807",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "extracted_data=load_pdf_file(data='Data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d4d258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Split the Data into Text Chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    text_chunks=text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1f5fef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Text Chunks 5859\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_chunks=text_split(extracted_data)\n",
    "print(\"Length of Text Chunks\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ddf298f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Om gunjal\\AppData\\Local\\Temp\\ipykernel_11392\\1787422007.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
      "c:\\Users\\Om gunjal\\.conda\\envs\\Medaibot\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def download_hugging_face_embeddings():\n",
    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    return embeddings\n",
    "\n",
    "embeddings = download_hugging_face_embeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0790d358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 384\n"
     ]
    }
   ],
   "source": [
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "print(\"Length\", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5a3b97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='33d7a780-9fe2-4586-a4c8-e742929f9f4a', metadata={'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'creator': 'PyPDF', 'creationdate': '2004-12-18T17:00:02-05:00', 'moddate': '2004-12-18T16:15:31-06:00', 'source': 'Data\\\\Medical_book.pdf', 'total_pages': 637, 'page': 39, 'page_label': '40'}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 226\\nAcne\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 26'), Document(id='687ee542-f7e2-447d-9c16-bcd376817541', metadata={'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'creator': 'PyPDF', 'creationdate': '2004-12-18T17:00:02-05:00', 'moddate': '2004-12-18T16:15:31-06:00', 'source': 'Data\\\\Medical_book.pdf', 'total_pages': 637, 'page': 38, 'page_label': '39'}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 2 25\\nAcne\\nAcne vulgaris affecting a woman’s face. Acne is the general\\nname given to a skin disorder in which the sebaceous\\nglands become inflamed. (Photograph by Biophoto Associ-\\nates, Photo Researchers, Inc. Reproduced by permission.)\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 25'), Document(id='c52c48df-34a1-4d71-84b1-db8b859e10e4', metadata={'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'creator': 'PyPDF', 'creationdate': '2004-12-18T17:00:02-05:00', 'moddate': '2004-12-18T16:15:31-06:00', 'source': 'Data\\\\Medical_book.pdf', 'total_pages': 637, 'page': 37, 'page_label': '38'}, page_content='Acidosis see Respiratory acidosis; Renal\\ntubular acidosis; Metabolic acidosis\\nAcne\\nDefinition\\nAcne is a common skin disease characterized by\\npimples on the face, chest, and back. It occurs when the\\npores of the skin become clogged with oil, dead skin\\ncells, and bacteria.\\nDescription\\nAcne vulgaris, the medical term for common acne, is\\nthe most common skin disease. It affects nearly 17 million\\npeople in the United States. While acne can arise at any')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Om gunjal\\.conda\\envs\\Medaibot\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Om gunjal\\.cache\\huggingface\\hub\\models--google--flan-t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n",
      "C:\\Users\\Om gunjal\\AppData\\Local\\Temp\\ipykernel_11392\\1371080727.py:37: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=pipe)\n",
      "C:\\Users\\Om gunjal\\AppData\\Local\\Temp\\ipykernel_11392\\1371080727.py:45: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm(prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skin disorder in which the sebaceous glands become inflamed\n"
     ]
    }
   ],
   "source": [
    "# ✅ Embeddings\n",
    "# ✅ Embeddings (keep as is)\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# ✅ Vector DB: FAISS\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Example: You must define `text_chunks` first. Example placeholder:\n",
    "# from langchain_community.document_loaders import PyPDFLoader\n",
    "# loader = PyPDFLoader(\"yourfile.pdf\")\n",
    "# documents = loader.load()\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "# text_chunks = splitter.split_documents(documents)\n",
    "\n",
    "# Create vectorstore\n",
    "vectorstore = FAISS.from_documents(documents=text_chunks, embedding=embeddings)\n",
    "\n",
    "# Use as retriever\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "retrieved_docs = retriever.invoke(\"What is Acne?\")\n",
    "print(retrieved_docs)\n",
    "\n",
    "# ✅ Local LLM using Hugging Face (use flan-t5-small instead of Falcon)\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "model_name = \"google/flan-t5-small\"  # ✅ Change here\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model.to(\"cpu\")\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_length=500)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Example usage\n",
    "question = \"What is Acne?\"\n",
    "context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "prompt = f\"Answer the following question using the context below:\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "\n",
    "response = llm(prompt)\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2296cd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔁 Loading existing FAISS index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🩺 Answer:\n",
      "a skin disorder in which the sebaceous glands become inflamed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "from src.prompt import system_prompt\n",
    "\n",
    "# --------------------------\n",
    "# 📄 1️⃣ PDF file to load\n",
    "# --------------------------\n",
    "# 🔥 CHANGE THIS to your PDF path if needed\n",
    "loader = PyPDFLoader(\"Data/Medical_book.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "# --------------------------\n",
    "# ✂️ 2️⃣ Text chunk settings\n",
    "# --------------------------\n",
    "# You can change chunk_size and chunk_overlap\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "text_chunks = text_splitter.split_documents(pages)\n",
    "\n",
    "# --------------------------\n",
    "# 💬 3️⃣ Embedding model\n",
    "# --------------------------\n",
    "# You can change model_name here if needed\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# --------------------------\n",
    "# 💾 4️⃣ FAISS index folder\n",
    "# --------------------------\n",
    "# You can change folder name here if you like\n",
    "index_folder = \"faiss_index\"\n",
    "\n",
    "if os.path.exists(index_folder):\n",
    "    print(\"🔁 Loading existing FAISS index...\")\n",
    "    vectorstore = FAISS.load_local(index_folder, embeddings, allow_dangerous_deserialization=True)\n",
    "else:\n",
    "    print(\"💥 Creating new FAISS index...\")\n",
    "    vectorstore = FAISS.from_documents(documents=text_chunks, embedding=embeddings)\n",
    "    vectorstore.save_local(index_folder)\n",
    "    print(\"✅ Index saved locally.\")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 🔍 5️⃣ Create retriever\n",
    "# --------------------------\n",
    "# You can change k (how many similar docs to return)\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "# --------------------------\n",
    "# 🤖 6️⃣ LLM model\n",
    "# --------------------------\n",
    "# CHANGE model_name to any local model you want (e.g., \"google/flan-t5-base\")\n",
    "model_name = \"google/flan-t5-small\"   #model_name = \"google/flan-t5-base\"  # ✅ Change here for fast OP\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model.to(\"cpu\")  # or \"cuda\" if using GPU\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_length=500, temperature=0.3)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# --------------------------\n",
    "# 💬 7️⃣ Example question\n",
    "# --------------------------\n",
    "# CHANGE question text below\n",
    "question = \"What is Acne?\"\n",
    "\n",
    "retrieved_docs = retriever.invoke(question)\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "context = context[:1500]\n",
    "\n",
    "\n",
    "\n",
    "prompt = (\n",
    "    system_prompt\n",
    "    + \"\\n\\n\"\n",
    "    + f\"{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    ")\n",
    "\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(\"🩺 Answer:\")\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Medaibot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
